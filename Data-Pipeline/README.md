# CourseWeave AI â€” Data Pipeline ğŸ“

This directory contains the complete data pipeline for CourseWeave AI, built using Apache Airflow, DVC, and Google Cloud Platform.

---

## ğŸ“ Folder Structure

```
Data-Pipeline/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ pipeline_dag.py        # Master Airflow DAG â€” orchestrates all tasks
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Raw scraped course data (CSV files)
â”‚   â”œâ”€â”€ processed/             # Cleaned, validated data + reports
â”‚   â””â”€â”€ temp/                  # Temporary files between pipeline tasks
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ acquire_data.py        # Loads raw CSV data into DataFrames
â”‚   â”œâ”€â”€ preprocess_data.py     # Cleans and normalizes data
â”‚   â”œâ”€â”€ validate_data.py       # Schema validation + statistics generation
â”‚   â”œâ”€â”€ detect_anomalies.py    # Detects data anomalies in PostgreSQL
â”‚   â”œâ”€â”€ load_data.py           # Loads cleaned data into PostgreSQL
â”‚   â”œâ”€â”€ db_config.py           # PostgreSQL connection configuration
â”‚   â”œâ”€â”€ web-extract.py         # Scrapes NEU course catalog website
â”‚   â””â”€â”€ pdf-extract.py         # Extracts data from PDF course syllabi
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_acquire_data.py   # Unit tests for data acquisition
â”‚   â”œâ”€â”€ test_preprocess_data.py # Unit tests for preprocessing
â”‚   â””â”€â”€ test_validate_data.py  # Unit tests for validation
â”œâ”€â”€ logs/                      # Pipeline execution logs
â”œâ”€â”€ dvc.yaml                   # DVC pipeline stages definition
â””â”€â”€ README.md                  # This file
```

---

## ğŸ”„ Pipeline Flow

```
NEU Website + PDFs
      â†“
web-extract.py + pdf-extract.py (Data Acquisition)
      â†“
GCS Bucket (courseweave-ai-data)
      â†“
Task 1: acquire_data     â†’ Downloads from GCS â†’ local data/raw/
      â†“
Task 2: preprocess_data  â†’ Cleans, normalizes, deduplicates
      â†“
Task 3: validate_data    â†’ Schema checks + statistics report
      â†“
Task 4: detect_anomalies â†’ SQL checks on PostgreSQL
      â†“
Task 5: bias_detection   â†’ Program coverage + Fairlearn analysis
      â†“
Task 6: dvc_versioning   â†’ Versions data to GCS remote
      â†“
Task 7: load_data        â†’ Loads to Cloud SQL PostgreSQL
      â†“
Task 8: pipeline_report  â†’ Final summary saved to GCS
```

---

## âš™ï¸ Setup Instructions

### 1. Clone the Repository
```bash
git clone https://github.com/NagashreeBK98/courseweave-ai-fork.git
cd courseweave-ai-fork/Data-Pipeline
```

### 2. Create Virtual Environment
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install Dependencies
```bash
pip install -r ../requirements.txt
```

### 4. Configure Environment Variables
```bash
cp ../.env.example ../.env
```

Edit `.env` with your credentials:
```
SLACK_WEBHOOK_URL=your-slack-webhook-url
GOOGLE_APPLICATION_CREDENTIALS=/path/to/your/service-account.json
GCP_PROJECT_ID=courseweave-ai
GCS_BUCKET_NAME=courseweave-ai-data
DB_HOST=your-cloud-sql-host
DB_PORT=5432
DB_NAME=courseweave
DB_USER=your-db-username
DB_PASSWORD=your-db-password
PINECONE_API_KEY=your-pinecone-api-key
PINECONE_INDEX_NAME=courseweave-ai
```

### 5. Set Up DVC Remote
```bash
export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
dvc remote add -d gcsremote gs://courseweave-ai-data
dvc pull
```

### 6. Initialize Airflow
```bash
export AIRFLOW_HOME=$(pwd)/../airflow-home
export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
airflow db migrate
airflow standalone
```

### 7. Access Airflow UI
```
http://localhost:8080
```
- Username: `admin`
- Password: Check `airflow-home/simple_auth_manager_passwords.json.generated`

---

## ğŸš€ Running the Pipeline

### Option 1 â€” Airflow UI
1. Go to `http://localhost:8080`
2. Find `courseweave_data_pipeline`
3. Click **â–¶ï¸ Trigger**
4. Monitor all 8 tasks

### Option 2 â€” Command Line
```bash
export AIRFLOW_HOME=$(pwd)/../airflow-home
airflow dags trigger courseweave_data_pipeline
```

### Option 3 â€” Run Scripts Individually
```bash
cd scripts/
python acquire_data.py
python preprocess_data.py
python validate_data.py
python detect_anomalies.py
python load_data.py
```

---

## ğŸ§ª Running Tests

```bash
# Run all tests
pytest tests/ -v

# Run specific test file
pytest tests/test_preprocess_data.py -v

# Run with coverage
pytest tests/ --cov=scripts -v
```

**Expected output:**
```
23 passed in 1.31s
```

---

## ğŸ“Š Data Versioning with DVC

### Check DVC Status
```bash
dvc status
```

### Pull Latest Data
```bash
dvc pull
```

### Push Data to GCS
```bash
dvc push
```

### Reproduce Pipeline
```bash
dvc repro
```

---

## ğŸ“ˆ Data Schema & Statistics

After each pipeline run, statistics are automatically generated and saved to:
- Local: `data/processed/stats_report.json`
- GCS: `gs://courseweave-ai-data/processed/stats_report.json`

**Sample statistics:**
```json
{
  "total_courses": 25,
  "by_program": {
    "MS_DAE": 12,
    "MS_DS": 4,
    "MS_CS": 5,
    "MS_DA": 2,
    "MS_IS": 2
  },
  "avg_credits": 4.0,
  "unique_programs": 5
}
```

---

## ğŸ” Bias Detection

The pipeline performs data slicing using **Fairlearn** to detect bias across programs:

### What is checked:
1. **Program Coverage** â€” Are all programs equally represented?
   - Flags programs with < 10% of total courses
   - Flags programs with 0 courses
2. **Credit Distribution** â€” Are credits fairly distributed?
   - Flags if credit average differs by > 1 across programs
3. **Fairlearn MetricFrame** â€” Course coverage by program group

### Mitigation Steps:
- If LOW COVERAGE detected â†’ Add more courses for underrepresented programs
- If CREDIT IMBALANCE detected â†’ Review credit allocation across programs
- All bias flags sent to Slack `#pipeline-alerts` channel

### Bias Report Location:
- Local: `data/processed/bias_report.json`
- GCS: `gs://courseweave-ai-data/processed/bias_report.json`

---

## ğŸš¨ Anomaly Detection

Two SQL-based checks run against PostgreSQL after each pipeline run:

1. **Missing Prerequisites** â€” Students who completed courses without meeting prerequisites
2. **Circular Prerequisites** â€” Course A requires B, B requires A (impossible loop)

Alerts sent to Slack if anomalies detected!

---

## ğŸ“¡ Monitoring & Alerts

### Slack Alerts
All pipeline events send Slack notifications:
- âœ… Task success messages
- ğŸš¨ Task failure alerts
- âš ï¸ Bias detection flags
- ğŸ‰ Pipeline completion summary

### Airflow Monitoring
- Task logs: `airflow-home/logs/`
- Gantt chart: Available in Airflow UI
- Task duration tracking

### GCS Error Logs
- Web scraping errors: `gs://courseweave-ai-data/error_logs/`
- PDF extraction errors: `gs://courseweave-ai-data/error_logs/`

---

## ğŸ”§ Troubleshooting

### GCP Credentials Error
```bash
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/service-account.json"
```

### DVC Push Failed
```bash
dvc remote modify gcsremote credentialpath /path/to/service-account.json
dvc push
```

### Airflow DAG Not Found
```bash
airflow config get-value core dags_folder
```

### Database Connection Failed
```bash
cat .env | grep DB
```

---

## ğŸ‘¥ Team

- Sachin Sreekumar
- Nagashree Bommenahalli Kumaraswamy
- Kavin Priyadarrsan Murugesan
- Siddharth Mohapatra
- Vigneshwaran Jayaraman
- Jogeashwini Srinivasan Ramesh

---

## ğŸ“ Course

IE 7374 â€” Machine Learning Operations
Northeastern University, Spring 2026